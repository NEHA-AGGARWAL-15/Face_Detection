{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876a85f3-0f9f-4f7e-904e-e2cc2e2da73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained VGG16 model (or any other model)\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Function to extract features and IDs from a set of images with the same ID\n",
    "def extract_features_with_id(image_paths, image_id):\n",
    "    all_features = []\n",
    "    all_ids = []\n",
    "    for path in image_paths:\n",
    "        img = image.load_img(path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        features = base_model.predict(img_array).flatten()\n",
    "        all_features.append(features)\n",
    "        all_ids.append(image_id)\n",
    "    return np.array(all_features), np.array(all_ids)\n",
    "\n",
    "# Sample paths to sets of images with respective IDs\n",
    "images_set1 = ['C:/Users/Shan/Desktop/img_processing/images.jpg', 'C:/Users/Shan/Desktop/img_processing/images_1.jpg', 'C:/Users/Shan/Desktop/img_processing/images_2.jpg']\n",
    "images_set2 = ['C:/Users/Shan/Desktop/img_processing/images4.jpg', 'C:/Users/Shan/Desktop/img_processing/images5.jpg', 'C:/Users/Shan/Desktop/img_processing/images6.jpg']\n",
    "ids = [1, 2]  # IDs corresponding to each set of images\n",
    "\n",
    "# Extract features and IDs for each set of images\n",
    "features_set1, ids_set1 = extract_features_with_id(images_set1, ids[0])\n",
    "features_set2, ids_set2 = extract_features_with_id(images_set2, ids[1])\n",
    "\n",
    "# feature_set1 = [1,2,4,5]\n",
    "# feature_set2 = [1,2,4,5,7,8,9,6]\n",
    "# id_set1 = [1,1,1,1]\n",
    "# id_set2 = [1,1,1,1,2,2,2,2]\n",
    "\n",
    "# Concatenate the features and IDs from different sets\n",
    "all_features = np.concatenate((features_set1, features_set2))\n",
    "all_ids = np.concatenate((ids_set1, ids_set2))\n",
    "\n",
    "# Train a logistic regression model to predict discrete IDs\n",
    "regression_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "regression_model.fit(all_features, all_ids)\n",
    "\n",
    "# Save the updated model\n",
    "joblib.dump(regression_model, 'base_model.pkl')\n",
    "\n",
    "# Path to the new image for prediction\n",
    "new_image_path = 'C:/Users/Shan/Desktop/img_processing/images4.jpg'  # Replace this with your new image path\n",
    "\n",
    "# Extract features from the new image\n",
    "new_image_features = base_model.predict(preprocess_input(np.expand_dims(image.img_to_array(image.load_img(new_image_path, target_size=(224, 224))), axis=0)))[0]\n",
    "\n",
    "# Predict the ID of the new image using the regression model\n",
    "predicted_id = regression_model.predict(new_image_features.reshape(1, -1))[0]\n",
    "\n",
    "print(f\"Predicted ID for the new image: {predicted_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df6ab8-e0f3-4519-9071-c0fa3b3c4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.predict(new_image_features.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ab77cd4-d204-4976-bd8d-5acdf00096f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Shan\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shan\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Shan\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "1/1 [==============================] - 1s 978ms/step\n",
      "1/1 [==============================] - 1s 504ms/step\n",
      "1/1 [==============================] - 0s 470ms/step\n",
      "1/1 [==============================] - 0s 455ms/step\n",
      "1/1 [==============================] - 0s 471ms/step\n",
      "1/1 [==============================] - 0s 443ms/step\n",
      "1/1 [==============================] - 0s 486ms/step\n",
      "Predicted ID for the new image: 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained VGG16 model (or any other model)\n",
    "base_model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "# Function to extract features and IDs from a set of images with the same ID\n",
    "def extract_features_with_id(image_paths, image_id):\n",
    "    all_features = []\n",
    "    all_ids = []\n",
    "    for path in image_paths:\n",
    "        img = image.load_img(path, target_size=(224, 224))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array = preprocess_input(img_array)\n",
    "        features = base_model.predict(img_array).flatten()\n",
    "        all_features.append(features)\n",
    "        all_ids.append(image_id)\n",
    "    return np.array(all_features), np.array(all_ids)\n",
    "\n",
    "# Sample paths to sets of images with respective IDs\n",
    "images_set1 = ['C:/Users/Shan/Desktop/img_processing/images.jpg', 'C:/Users/Shan/Desktop/img_processing/images_1.jpg', 'C:/Users/Shan/Desktop/img_processing/images_2.jpg']\n",
    "images_set2 = ['C:/Users/Shan/Desktop/img_processing/images4.jpg', 'C:/Users/Shan/Desktop/img_processing/images5.jpg', 'C:/Users/Shan/Desktop/img_processing/images6.jpg']\n",
    "ids = [1, 2]  # IDs corresponding to each set of images\n",
    "\n",
    "# Extract features and IDs for each set of images\n",
    "features_set1, ids_set1 = extract_features_with_id(images_set1, ids[0])\n",
    "features_set2, ids_set2 = extract_features_with_id(images_set2, ids[1])\n",
    "\n",
    "# Concatenate the features and IDs from different sets\n",
    "all_features = np.concatenate((features_set1, features_set2))\n",
    "all_ids = np.concatenate((ids_set1, ids_set2))\n",
    "\n",
    "# Train a logistic regression model to predict discrete IDs\n",
    "regression_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "regression_model.fit(all_features, all_ids)\n",
    "\n",
    "# Save the base model\n",
    "joblib.dump(regression_model, 'base_model.pkl')\n",
    "\n",
    "# Path to the new image for prediction\n",
    "new_image_path = 'C:/Users/Shan/Desktop/img_processing/images4.jpg'  # Replace this with your new image path\n",
    "\n",
    "# Extract features from the new image\n",
    "new_image_features = base_model.predict(preprocess_input(np.expand_dims(image.img_to_array(image.load_img(new_image_path, target_size=(224, 224))), axis=0)))[0]\n",
    "\n",
    "# Predict the ID of the new image using the regression model\n",
    "predicted_id = regression_model.predict(new_image_features.reshape(1, -1))[0]\n",
    "\n",
    "print(f\"Predicted ID for the new image: {predicted_id}\")\n",
    "\n",
    "# # Sample paths to new set of images\n",
    "# new_images = ['C:/Users/Shan/Desktop/img_processing/images7.jpg', 'C:/Users/Shan/Desktop/img_processing/images8.jpg', 'C:/Users/Shan/Desktop/img_processing/images9.jpg']\n",
    "# new_image_id = 3  # ID corresponding to the new set of images\n",
    "\n",
    "# # Extract features and IDs for the new set of images\n",
    "# features_set3, ids_set3 = extract_features_with_id(new_images, new_image_id)  # Extract features for the new set\n",
    "\n",
    "# # Load the existing model\n",
    "# existing_model = joblib.load('base_model.pkl')  # Load your saved model\n",
    "\n",
    "# # Concatenate the existing model's features and IDs with the new set\n",
    "# updated_features = np.concatenate((existing_model.coef_, features_set3))\n",
    "# updated_ids = np.concatenate((existing_model.intercept_ * np.ones(len(ids_set3)), ids_set3))  # Adjust the IDs for the new set\n",
    "\n",
    "# # Retrain the logistic regression model with the updated data\n",
    "# updated_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# # Use only the features and IDs of the new set, without including existing model's coefficients and intercept\n",
    "# updated_model.fit(np.concatenate((all_features, features_set3)), np.concatenate((all_ids, ids_set3)))\n",
    "\n",
    "# # Save the updated model\n",
    "# joblib.dump(updated_model, 'base_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cffe8fc2-c993-4329-a2c4-36d29caa53a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 431ms/step\n",
      "1/1 [==============================] - 0s 439ms/step\n",
      "1/1 [==============================] - 0s 435ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['base_model.pkl']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample paths to new set of images\n",
    "new_images = ['C:/Users/Shan/Desktop/img_processing/images10.jpg', 'C:/Users/Shan/Desktop/img_processing/images11.jpg', 'C:/Users/Shan/Desktop/img_processing/images12.jpg']\n",
    "new_image_id = 4  # ID corresponding to the new set of images\n",
    "\n",
    "# Extract features and IDs for the new set of images\n",
    "features_set3, ids_set3 = extract_features_with_id(new_images, new_image_id)  # Extract features for the new set\n",
    "\n",
    "# Load the existing model\n",
    "existing_model = joblib.load('base_model.pkl')  # Load your saved model\n",
    "\n",
    "# Concatenate the existing model's features and IDs with the new set\n",
    "updated_features = np.concatenate((existing_model.coef_, features_set3))\n",
    "updated_ids = np.concatenate((existing_model.intercept_ * np.ones(len(ids_set3)), ids_set3))  # Adjust the IDs for the new set\n",
    "\n",
    "# Retrain the logistic regression model with the updated data\n",
    "updated_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Use only the features and IDs of the new set, without including existing model's coefficients and intercept\n",
    "updated_model.fit(np.concatenate((all_features, features_set3)), np.concatenate((all_ids, ids_set3)))\n",
    "\n",
    "# Save the updated model\n",
    "joblib.dump(updated_model, 'base_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd27da88-1009-4467-8605-921fd1ad053a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 496ms/step\n",
      "Predicted ID for the new image: 1\n"
     ]
    }
   ],
   "source": [
    "# loading the updated model to test fetching\n",
    "new_model = joblib.load('base_model.pkl')\n",
    "\n",
    "# path for latest feed images\n",
    "new_image_path = 'C:/Users/Shan/Desktop/img_processing/images_2.jpg'  # Replace this with your new image path\n",
    "\n",
    "# Extract features from the new image\n",
    "new_image_features = base_model.predict(preprocess_input(np.expand_dims(image.img_to_array(image.load_img(new_image_path, target_size=(224, 224))), axis=0)))[0]\n",
    "\n",
    "# Predict the ID of the new image using the loaded model\n",
    "predicted_id = new_model.predict(new_image_features.reshape(1, -1))[0]\n",
    "\n",
    "print(f\"Predicted ID for the new image: {predicted_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08343901-0ce2-41cc-8484-5be51b3ecf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 449ms/step\n",
      "The image does not match the expected accuracy threshold.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Function to predict ID for a new image based on the trained model\n",
    "def predict_id(model, image_features, true_id, threshold=0.7):\n",
    "    predicted_id = model.predict(image_features.reshape(1, -1))[0]\n",
    "    accuracy = accuracy_score([true_id], [predicted_id])\n",
    "    \n",
    "    if accuracy >= threshold:\n",
    "        return predicted_id\n",
    "    else:\n",
    "        return float('nan')\n",
    "\n",
    "# Load the trained model\n",
    "trained_model = joblib.load('base_model.pkl')\n",
    "\n",
    "# Path to the new image for prediction\n",
    "new_image_path = 'C:/Users/Shan/Desktop/img_processing/images_1.jpg'  # Replace this with your new image path\n",
    "\n",
    "# Extract features from the new image\n",
    "new_image_features = base_model.predict(preprocess_input(np.expand_dims(image.img_to_array(image.load_img(new_image_path, target_size=(224, 224))), axis=0)))[0]\n",
    "\n",
    "# Predict the ID of the new image using the trained model and check accuracy\n",
    "predicted_id = predict_id(trained_model, new_image_features, new_image_id)\n",
    "\n",
    "if not np.isnan(predicted_id):\n",
    "    print(f\"Predicted ID for the new image: {predicted_id}\")\n",
    "else:\n",
    "    print(\"The image does not match the expected accuracy threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ead176-b7f2-4234-b70d-c85a54f55c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3454a2a0-f2e8-4918-8a96-fdf9db69af9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3533ee67-b768-477b-9f42-26f177ee2786",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3e29a9a8-51d0-4529-8374-108bd7445b7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1/1 [==============================] - 4s 4s/step - loss: 24.7632 - accuracy: 0.0000e+00 - val_loss: -143.6735 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 3s 3s/step - loss: -50.3571 - accuracy: 0.7500 - val_loss: -264.3690 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -65.9712 - accuracy: 0.7500 - val_loss: -375.5847 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -131.0070 - accuracy: 0.7500 - val_loss: -485.2854 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -192.1383 - accuracy: 0.7500 - val_loss: -599.2549 - val_accuracy: 0.0000e+00\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -179.9019 - accuracy: 0.7500 - val_loss: -711.7045 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -280.1385 - accuracy: 0.7500 - val_loss: -825.5115 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -371.6816 - accuracy: 0.7500 - val_loss: -943.3221 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -489.4359 - accuracy: 0.7500 - val_loss: -1063.5212 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: -423.8935 - accuracy: 0.7500 - val_loss: -1183.0029 - val_accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:5 out of the last 26 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA5CCCFBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 26 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001EA5CCCFBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m predicted_id \u001b[38;5;241m=\u001b[39m  predict_id(model, new_image_path)\n\u001b[0;32m     68\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score([new_image_id], [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m predicted_id \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m---> 69\u001b[0m predicted_id \u001b[38;5;241m=\u001b[39m true_id \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mthreshold\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(predicted_id[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted ID for the new image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_id[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the pre-trained VGG16 model without the top layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Freeze the pre-trained layers to prevent retraining\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define a new model architecture\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(lr=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Sample paths to sets of images with respective IDs\n",
    "images_set1 = ['C:/Users/Shan/Desktop/img_processing/images.jpg', 'C:/Users/Shan/Desktop/img_processing/images_1.jpg', 'C:/Users/Shan/Desktop/img_processing/images_2.jpg']\n",
    "images_set2 = ['C:/Users/Shan/Desktop/img_processing/images4.jpg', 'C:/Users/Shan/Desktop/img_processing/images5.jpg', 'C:/Users/Shan/Desktop/img_processing/images6.jpg']\n",
    "ids = [1, 2]  # IDs corresponding to each set of images\n",
    "\n",
    "# Combine image paths and IDs for training data\n",
    "all_image_paths = images_set1 + images_set2\n",
    "all_labels = np.array([ids[0]] * len(images_set1)+[ids[1]] * len(images_set2))  # Repeated IDs for corresponding images\n",
    "\n",
    "# Function to preprocess image paths into trainable format\n",
    "def preprocess_image(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "# Convert all images to trainable format\n",
    "all_images = np.concatenate([preprocess_image(img_path) for img_path in all_image_paths])\n",
    "\n",
    "# Train the model\n",
    "model.fit(all_images, all_labels, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Save the model\n",
    "model.save('enhanced_model.h5')\n",
    "\n",
    "# Function to predict ID for a new image based on the trained model\n",
    "def predict_id(model, img_path, threshold=0.7):\n",
    "    img = preprocess_image(img_path)\n",
    "    predicted_id = model.predict(img)\n",
    "    \n",
    "    return \n",
    "\n",
    "# Path to the new image for prediction\n",
    "new_image_path = 'C:/Users/Shan/Desktop/img_processing/images4.jpg'  # Replace this with your new image path\n",
    "new_image_id = 2  # Replace this with the ID corresponding to the new image\n",
    "\n",
    "# Predict the ID of the new image using the trained model and check accuracy\n",
    "predicted_id =  predict_id(model, new_image_path)\n",
    "accuracy = accuracy_score([new_image_id], [1 if predicted_id > threshold else 0])\n",
    "predicted_id = true_id if accuracy >= threshold else float('nan')\n",
    "\n",
    "if not np.isnan(predicted_id[0]):\n",
    "    print(f\"Predicted ID for the new image: {predicted_id[0]}\")\n",
    "else:\n",
    "    print(\"The image does not match the expected accuracy threshold.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74879082-0607-4854-8bf0-c031158a8c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([ids[0]] * len(images_set1)+[ids[1]] * len(images_set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59b97152-f45e-4532-8082-dffff3fe1b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_id\n",
    "# accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e46f5728-7f80-436e-bd05-85e3c0b9c8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_image_id=3\n",
    "new_images = ['C:\\\\Users\\\\Shan\\\\Documents\\\\projects\\\\CrimiDatabase\\\\media\\\\uploads\\\\images10_n1ydnrw.jpg', 'C:\\\\Users\\\\Shan\\\\Documents\\\\projects\\\\CrimiDatabase\\\\media\\\\uploads\\\\images11_MLIdqHO.jpg', 'C:\\\\Users\\\\Shan\\\\Documents\\\\projects\\\\CrimiDatabase\\\\media\\\\uploads\\\\images12_YQOU4L6.jpg', 'C:\\\\Users\\\\Shan\\\\Documents\\\\projects\\\\CrimiDatabase\\\\media\\\\uploads\\\\images9_v7A3g4t.jpg', 'C:\\\\Users\\\\Shan\\\\Documents\\\\projects\\\\CrimiDatabase\\\\media\\\\uploads\\\\images8_utEE7NK.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e66bc777-7ee6-431c-adf7-31dddfa4ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 1s 959ms/step\n",
      "1/1 [==============================] - 1s 907ms/step\n",
      "1/1 [==============================] - 1s 834ms/step\n",
      "1/1 [==============================] - 1s 836ms/step\n"
     ]
    }
   ],
   "source": [
    "features_set3, ids_set3 = extract_features_with_id(new_images, new_image_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b457b1fe-dbdd-4e3f-ac75-875b70a42dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25088)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_set3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bf5fd82-dff5-447f-b3c7-88ae03ce913c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d9158e2-bf35-47cf-9cf8-d98233840169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# features_set3, ids_set3 = extract_features_with_id(new_images, new_image_id)  # Extract features for the new set\n",
    "\n",
    "# Load the existing model\n",
    "existing_model = joblib.load('base_model.pkl')  # Load your saved model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eefd5254-8ddc-4cbf-a1ab-8e14051ddf00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 25088)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the existing model's features and IDs with the new set\n",
    "updated_features = np.concatenate((existing_model.coef_, features_set3))\n",
    "updated_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "40e26b67-6931-4f04-8fc3-d78b8ba32a02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_ids = np.concatenate((existing_model.intercept_ * np.ones(len(ids_set3)), ids_set3))  # Adjust the IDs for the new set\n",
    "updated_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9e3f9064-f9f8-4c71-9a81-6ccce9ab8b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25088)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_model.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3988db55-a4b7-4eba-929f-8741de040e8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [11, 15]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m updated_model \u001b[38;5;241m=\u001b[39m LogisticRegression(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlbfgs\u001b[39m\u001b[38;5;124m'\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Use only the features and IDs of the new set, without including existing model's coefficients and intercept\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mupdated_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupdated_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures_set3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids_set3\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Save the updated model\u001b[39;00m\n\u001b[0;32m     12\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(updated_model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbase_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1208\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1206\u001b[0m     _dtype \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mfloat64, np\u001b[38;5;241m.\u001b[39mfloat32]\n\u001b[1;32m-> 1208\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msolver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mliblinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msag\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msaga\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y)\n",
      "File \u001b[1;32m~\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\sklearn\\base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    620\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    621\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 622\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    623\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1146\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1147\u001b[0m     X,\n\u001b[0;32m   1148\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1160\u001b[0m )\n\u001b[0;32m   1162\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m-> 1164\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, y\n",
      "File \u001b[1;32m~\\Documents\\projects\\CrimiDatabase\\venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    405\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    409\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    410\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [11, 15]"
     ]
    }
   ],
   "source": [
    "# Retrain the logistic regression model with the updated data\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "lab = preprocessing.LabelEncoder()\n",
    "y_transformed = lab.fit_transform(updated_ids)\n",
    "\n",
    "updated_model = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "# Use only the features and IDs of the new set, without including existing model's coefficients and intercept\n",
    "updated_model.fit(np.concatenate((updated_features, features_set3)), np.concatenate((y_transformed, ids_set3)))\n",
    "\n",
    "# Save the updated model\n",
    "joblib.dump(updated_model, 'base_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e653d-4c5d-4bb3-a2f8-0853a4442585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
